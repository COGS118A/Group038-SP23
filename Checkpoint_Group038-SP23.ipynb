{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118A - Project Checkpoint - Group 38"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "- Bruce Zhou\n",
    "- Yifei Wang\n",
    "- Qiyun Li\n",
    "- Michael Sherrick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "In this project, we aim to construct a machine learning model capable of predicting diabetes in patients using a dataset sourced from Kaggle. The dataset contains 100,000 observations, each characterized by 8 variables including age, gender, smoking history, and more. The pandas library will be utilized to streamline data preprocessing and attribute selection. Several predictive models, namely k-Nearest Neighbors (knn), Support Vector Machines (svm), and Decision Trees, will be employed to build the predictive system. These models will undergo fine-tuning using the grid search cross-validation method to optimize their performance. The effectiveness of the final model will be gauged by various performance metrics, including accuracy, recall, and the F1 score. This comprehensive evaluation will ensure the model's robustness and reliability in predicting diabetes. Our ultimate objective is to provide a powerful tool for early diabetes prediction, potentially enabling timely intervention and management of the disease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Diabetes is a global health problem affecting a significant percentage of the population worldwide, presenting substantial public health and economic burdens[<sup>[1]</sup>](#1) . Timely and accurate diabetes prediction is paramount in facilitating early intervention and management, which can significantly enhance patient outcomes and mitigate associated healthcare costs[<sup>[2]</sup>](#2). Machine learning has been recognized as an instrumental tool in predicting diabetes, harnessing the power of vast health data for refined disease prediction and management[<sup>[3]</sup>](#3). Various machine learning methods, including decision trees, k-nearest neighbors (knn), and support vector machines (svm), have been applied in this field[<sup>[4]</sup>](#4). Existing research has corroborated the efficacy of these machine learning models in predicting diabetes. Maniruzzaman et al. deployed decision trees, knn, and svm to predict diabetes and reported appreciable levels of accuracy[<sup>[5]</sup>](#5). Furthermore, Khan et al. emphasized the importance of optimizing these models using methods such as grid search for superior performance[<sup>[6]</sup>](#6).\n",
    "\n",
    "Despite these promising developments, certain areas require further exploration. Many of the previous studies have concentrated on smaller datasets, whereas applying machine learning to larger and more comprehensive datasets, like the one used in this study, could potentially enhance prediction accuracy. Additionally, a scarcity of studies have compared the performance of multiple models on the same dataset to identify the best model or combination of models for diabetes prediction. In view of these gaps in the existing literature, our project will deploy multiple machine learning techniques, including decision trees, knn, and svm, to a large diabetes dataset. We will also fine-tune these models using grid search cross-validation and assess their performance using various metrics to offer a comprehensive evaluation of their predictive ability. Our endeavor could provide valuable insights into the optimal machine learning methodologies for diabetes prediction, thereby bolstering early detection and intervention strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "Using machine learning, we aim to predict patients' diabetes status based on their healthy conditions. We will use the dataset with 60000 observations to build our ML system. The problem is quantifiable in the way that the outputs of our model will be either 1 or 0, where 1 stands for the presence of diabetes and 0 stands for the absence of diabetes. The problem is a typical binary classification problem, so our ML system can be measured with various metrics, including accuracy, recall, and f1 score. The problem is also replicable with new data as long as the data contains the required healthy conditions of patients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "In this project, we explored [Diabetes prediction dataset](https://www.kaggle.com/datasets/iammustafatz/diabetes-prediction-dataset)</a><sup>[1]</sup> provided by kaggle. The Diabetes prediction dataset is a collection of medical and demographic data from patients. This dataset contains 100,000 observations and 8 variables from patients with or without diabetes.\n",
    "\n",
    "Each observation consists of the patient's healthy conditions, including their age, gender, body mass index (BMI), hypertension, heart disease, smoking history, HbA1c level, and blood glucose level, along with their diabetes status. One sample observation is as following:\n",
    "\n",
    "```json\n",
    "{\n",
    "   \"gender\": \"Female\",\n",
    "    \"age\": 80.0,\n",
    "    \"hypertension\": 0,\n",
    "    \"heart_disease\": 1,\n",
    "    \"smoking_history\": \"never\",\n",
    "    \"bmi\": 25.19,\n",
    "    \"HbA1c_level\": 6.6,\n",
    "    \"blood glucose level\": 140,\n",
    "    \"diabetes\": 0\n",
    "}\n",
    "```\n",
    "\n",
    "The most critical variable is \"diabetes\", where 1 stands for the presence of diabetes and 0 for the absense of diabetes. Another critical variable is \"blood glucose level\", which is represented by a float.\n",
    "\n",
    "Before building our ML system, we transformed the categorical variables to numerical variables with one-hot-encoding. We also normalized numercial data with z-score normalizing. In the process of fiting and transforming data, we utilized pipeline from scikit learn to make sure that no test information was leaked in the trainning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "For the problem at hand, our proposed solution involves building a machine learning system that utilizes various algorithms to predict diabetes status based on patients' health conditions. With a dataset encompassing 60,000 observations, the model will be trained to generate binary outputs: 1 for the presence of diabetes and 0 for its absence.\n",
    "\n",
    "The machine learning algorithms to be employed in this project include k-Nearest Neighbors (knn), Support Vector Machines (svm), and Decision Trees. And here are the Algorithm description:\n",
    "\n",
    "| **Algorithm**             | **Description**   |\n",
    "|-----------------------|---------------|\n",
    "| k-Nearest Neighbors (knn) | This algorithm classifies a data point based on the majority class of its 'k' nearest neighbors in the training data. In our context, it can be used to classify a patient as diabetic or non-diabetic based on the diabetes status of the 'k' most similar patients in our dataset. |\n",
    "| Support Vector Machines (svm) | SVMs find the hyperplane that separates classes with the maximum margin in a high-dimensional feature space. In the diabetes prediction context, this means finding the hyperplane that best separates diabetic and non-diabetic patients based on their health conditions. |\n",
    "| Decision Trees | Decision trees make decisions by recursively partitioning data based on feature values. In our problem, a decision tree could be used to determine whether a patient is likely to have diabetes based on decisions made at each node, which correspond to health condition attributes. |\n",
    "\n",
    "The implementation of the solution will use Python and its robust libraries like pandas and scikit-learn. The sklearn's GridSearchCV function will be used for fine-tuning model parameters to improve the prediction accuracy. The output will be tested by error metrics. These metrics will provide a comprehensive view of model performance, allowing us to identify the model that delivers the best predictive performance. To ensure the robustness of our solution, we will compare our model to a benchmark model. The benchmark model will be a simple logistic regression model, which is a common choice for binary classification problems.\n",
    "\n",
    "For data selection, we follow the 70-15-15 rule: 70% of the data for training, 15% for validation, and the remaining 15% for testing. After dividing the dataset, we will train our models using the training data with methods above. To fine-tune the hyperparameters and avoid overfitting, we choose the grid search cross-validation method. The training set is randomly partitioned into 'k' equal-sized subsets. One of the subsets are used for validation, and the remaining subset is used for training. This process is repeated 'k' times, each time with a different validation subset, and the average performance metric is computed. The combination of hyperparameters with the best average performance will be selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "Our ML system will perform a typical binary classification task, where the final results can be represented in a confusion matrix:\n",
    "\n",
    "|                |          | **Prediction** |          |\n",
    "| -------------- | -------- | -------------- | -------- |\n",
    "|                |          | Positive       | Negative |\n",
    "| **True Label** | Positive | TP             | FN       |\n",
    "|                | Negative | FP             | TN       |\n",
    "\n",
    "We will use different matrices, including accuracy, recall, and f1 score. The corresponding mathematics representations are in the following:\n",
    "$$\n",
    "Accuracy = \\frac{TP+TN}{TP+FP+FN+TN}\\\\\n",
    "Recall = \\frac{TP}{TP+FN}\\\\\n",
    "F1 = 2*\\frac{Precision * Recall}{Precision + Recall}\\text{, where }Precision = \\frac{TP}{TP+FP}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary results\n",
    "For now, we have implmented two models to complete the task: KNN model and SVC model. For the Descision Tree(DT), we have already build the skeleton for that and we are working on tuning it, so it will not be showed in detail. \n",
    "### Data pre-process:\n",
    "* To make both categorical and normalized data readible, we use One hot encoding before training.\n",
    "* We choose 90% of all data use as training set, and among the training set, we divide it into training(80%) and validating(20%) sets.\n",
    "* The data for positive and negative cases are seriously unbalanced(about 1:9), therefore, we cannot use accuracy to evaulate the model. So the may metirc will be Recall value and F1-score. The unbalance of the data also reflected in the low recall score when we testing our SVC and KNN models.\n",
    "\n",
    "### Part1: SVC Model\n",
    "\n",
    "* We have first changed the categorical data to numerical data with one hot encoding and normalized the numerical data with z-score normalizing. Then we used pipeline to make sure that no test information was leaked in the trainning. It was notable that we have applied the one hot encoding on the gender and smoking cloumn only even if there are more categorical data in the dataset. The reason is that the other categorical data have already been represented by numerical data. For example, the hypertension column has already been represented by 0 and 1. The one hot encoding will only add more columns with 0 and 1. This will not help the model to perform better.\n",
    "* Validation confusion matrix with default setting of params: \n",
    "* ![img/default_svc_cm.png](img/default_svc_cm.png)\n",
    "* Validation matrices: \n",
    "\n",
    "|   Metric  |   Value    |\n",
    "|:---------:|:----------:|\n",
    "| Accuracy  |   0.9609   |\n",
    "| Recall    |   0.5927   |\n",
    "| F1 Score  |   0.7320   |\n",
    "\n",
    "* we used cross-validation and grid-search to find the best hyperparameter among 348 candidates and 1740 fits and the result is shown in table below\n",
    "\n",
    "| Parameter           | Value                   |\n",
    "|---------------------|-------------------------|\n",
    "| classifier__C       | 3.1622776601683795      |\n",
    "| classifier__gamma   | 0.03162277660168379     |\n",
    "| classifier__kernel  | rbf                     |\n",
    "\n",
    "\n",
    "* The we made some modification about down sampling the dataset to avoid the problem of unbalanced data. In the table, we showed how we change the ratio.\n",
    "\n",
    "|                       | Original Ratio | Down-Sampled Ratio |\n",
    "|-----------------------|----------------|--------------------|\n",
    "| Positive : Negative   |      1 : 9     |       3 : 7        |\n",
    "\n",
    "\n",
    "* Validation confusion matrix: \n",
    "* ![img/svc_cm.png](img/svc_cm.png)\n",
    "* Validation matrices after adjusting model: \n",
    "\n",
    "|   Metric  |   Value    |\n",
    "|:---------:|:----------:|\n",
    "| Accuracy  |   0.9417   |\n",
    "| Recall    |   0.7880   |\n",
    "| F1 Score  |   0.7089   |\n",
    "\n",
    "\n",
    "* After the modification our model shows better performance in classifying positive patiences and worse performance in classifying negative patiences. Thus we get higher recall and comparable accuracy & f1.\n",
    "\n",
    "### Part2: KNN Model\n",
    "* We used cross-validation to find the best k-value among 10 k's from k = 1 to k = 316. Among which we find the best k to be chosen is when k = 12.\n",
    "* Validation confusion matrix: \n",
    "* ![img/knn_cm.png](img/knn_cm.png)\n",
    "* Validation matrices for KNN model: \n",
    "\n",
    "|   Metric  |   Value    |\n",
    "|:---------:|:----------:|\n",
    "| Accuracy  |   0.9613   |\n",
    "| Recall    |   0.5971   |\n",
    "| F1 Score  |   0.7355   |\n",
    "\n",
    "* from the initial result, we find the recall value is significant low. After checking the data we find the positive: negative case is 1:9, the predictied data is seriously unbalanced. So although we have high accuracy, we cannot use that to evalulate the model(if we choose negative everytime, the accuracy can also reach above 90%).\n",
    "\n",
    "### Part3: Decision Tree model\n",
    "\n",
    "* we used cross-validation and grid-search to find the best hyperparameter, find that currently the best hyperparameter are:\n",
    "\n",
    "| Parameter                    | Value  |\n",
    "|------------------------------|--------|\n",
    "| classifier__criterion        | gini   |\n",
    "| classifier__max_depth        | 5      |\n",
    "| classifier__min_samples_leaf | 1      |\n",
    "| classifier__min_samples_split| 2      |\n",
    "\n",
    "* And we have the validation matricies:\n",
    "\n",
    "|   Metric  |   Value    |\n",
    "|:---------:|:----------:|\n",
    "| Accuracy  |   0.9692   |\n",
    "| Recall    |   0.7879   |\n",
    "| F1 Score  |   0.6500   |\n",
    "\n",
    "* This is the initial tuning for the DT model, and we will keep tuning the model in later time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy\n",
    "\n",
    "When engaging in data-driven machine learning, it is important to consider the ethical implications and privacy concerns associated with data usage. In the context of online review platforms, the data used can potentially expose personal information of individuals who provided reviews. To lower this risk, we have selected a dataset where no patient's personal information, such as name and contact info, is included.\n",
    "\n",
    "Additionally, it is crucial to ensure that any insights derived from the data analysis are free from biases or discrimination against specific groups or people. Transparency is key in presenting findings, and any potential biases in the data must be acknowledged and appropriately addressed.\n",
    "\n",
    "Furthermore, the data will be used exclusively for academic purposes, with no sharing or selling to third parties. Ultimately, our analysis is conducted with a responsible and ethical approach to avoid any potential harm to individuals or businesses involved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations\n",
    "- Weekly team meetings will be held to collectively determine the primary goal for the week.\n",
    "- Despite having different responsibilities, each team member is expected to contribute an approximately equal amount of time to the project overall.\n",
    "- It is crucial to ensure that there are no team members who do not actively contribute to the project. Any instances of this will be immediately reported.\n",
    "- All tasks must be completed within the agreed-upon deadline established during the weekly meeting, and there should be no unexplained delays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal\n",
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 5/14  |  Afternoon |  Nothing  | Familiarize ourselves with each other during the first in-person meeting. Engage in a discussion to identify and discuss the strengths and weaknesses of each group member. Find the dataset used in this project |\n",
    "| 5/17  |  whole day |  Investigate the dataset by looking at the summaries on the kaggle | Complete the project proposal. |\n",
    "| 5/20  | afternoon  | Preprocess the dataset in the notebook | First attempt to perform some data analysis on the dataset. Familiarize with the dataset. |\n",
    "| 5/27  | night  | Built the model for SVC and KNN | tuning the models |\n",
    "| 6/3  | evening | list methods to imporve the performance | pair-coding to realize the plan, and try to find the optimal options |\n",
    "| 6/10  | undetermined  | the project itself, we need to finish the majority of the project | details and polish the report |\n",
    "| 6/19  | Before 11:59 PM  | NA | Turn in Final Project  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"1\"></a>1.: World Health Organization. (2021). Diabetes. https://www.who.int/news-room/fact-sheets/detail/diabetes<br>\n",
    "<a name=\"2\"></a>2.: Khan, M. A., & Hashim, M. J. (2020). Global burden of diabetes. Diabetes Research and Clinical Practice. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7310804/.<br>\n",
    "<a name=\"3\"></a>3. : Darcy, A. M., Louie, A. K., & Roberts, L. W. (2016). Machine learning and the profession of medicine. JAMA, 315(6), 551-552. https://jamanetwork.com/journals/jama/article-abstract/2488315<br>\n",
    "<a name=\"4\"></a>4. : Kavakiotis, I., Tsave, O., Salifoglou, A., Maglaveras, N., Vlahavas, I., & Chouvarda, I. (2017). Machine learning and data mining methods in diabetes research. Computational and structural biotechnology journal, 15, 104-116. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5257026/ <br>\n",
    "<a name=\"5\"></a>5. : Maniruzzaman, M., Rahman, M. J., Al-MehediHasan, M., Suri, H. S., Abedin, M. M., El-Baz, A., & Suri, J. S. (2018). Accurate diabetes risk stratification using machine learning: role of missing value imputation and data balancing. Journal of Medical Systems, 42(4), 74. ↩ https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5893681/<br>\n",
    "<a name=\"6\"></a>6. : Khan, J., Wei, J. S., Ringnér, M., Saal, L. H., Ladanyi, M., Westermann, F., Berthold, F., Schwab, M., Antonescu, C. R., Peterson, C., & Meltzer, P. S. (2001). Classification and diagnostic prediction of cancers using gene expression profiling and artificial neural networks. Nature Medicine, 7(6), 673-679. ↩ https://www.ncbi.nlm.nih.gov/pmc/articles/pmid/11385503/<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
